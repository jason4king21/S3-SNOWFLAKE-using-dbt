# ğŸ§± End-to-End Data Pipeline with SCD1 & SCD2 using DBT, AWS S3, and Snowflake

This project demonstrates an end-to-end modern data pipeline using **dbt** to ingest data from **AWS S3** into **Snowflake**, structured into **Bronze**, **Silver**, and **Gold** layers. It also includes support for both **Slowly Changing Dimension Type 1 (SCD1)** and **Type 2 (SCD2)** logic using **dbt snapshots and models**, a **dynamic Snowflake stored procedure** to infer and load schemas, and a **Python notebook** that produces charts and tables for analysis directly within Snowflake.

---

## ğŸ§© Architecture Overview

![Data Pipeline Diagram](diagrams/architecture.png)

## ğŸ§© dbt Linage

![Data Pipeline Diagram](diagrams/dbt_linage.png)

1. **Raw File Drop**  
   - Source `.csv` files are uploaded into an S3 bucket.

2. **Ingestion via Stored Procedure**  
   - A Snowflake stored procedure dynamically infers the schema and loads raw data into the Bronze layer.

3. **DBT Layered Transformations**  
   - **Bronze Layer**: Raw ingested data from S3.  
   - **Silver Layer**: Cleaned and enriched data with SCD1 and SCD2 logic applied.  
   - **Gold Layer**: Curated views for analytics and dashboards.

4. **Data Lineage & Snapshots**  
   - dbt snapshots track history using SCD2 methodology.
   - Final views are exposed for reporting.

5. **Python Visualizations in Snowflake**  
   - Charts and tables are rendered using Snowflake Notebooks with Python.

---

## ğŸ”§ Tech Stack

- **AWS S3** â€“ Object store for source data files
- **Snowflake** â€“ Data warehouse and procedural logic execution
- **DBT** â€“ Modeling tool for transformation and versioning
- **Python** â€“ Used in Snowflake notebooks for generating graphs and pivot tables
- **Snowflake Stored Procedures** â€“ Dynamic schema inference and file load logic

---

## âš™ï¸ Stored Procedure: `LOAD_WALMART_DATA()`

A SQL stored procedure in the `BRONZE` schema dynamically ingests files staged in S3. It performs the following:

- Reads from a `control_table` to fetch filenames and MD5 hashes
- Uses `INFER_SCHEMA` to generate DDL if the table doesnâ€™t exist
- Executes `COPY INTO` to load data
- Adds metadata columns (`ingestion_timestamp`, `source_file_name`)
- Logs the file as imported in `imported_files` to avoid duplicates

---

## ğŸ“‚ Repository Structure
```
S3-SNOWFLAKE-using-dbt/
â”‚
â”œâ”€â”€ analyses/                         â†’ Placeholder for ad-hoc analysis files
â”‚
â”œâ”€â”€ diagrams/
â”‚   â”œâ”€â”€ architecture.png              â†’ Visual diagram of the data pipeline
â”‚   â””â”€â”€ Diagram Generator/
â”‚       â””â”€â”€ architecture.py           â†’ Python script to generate the architecture diagram
â”‚
â”œâ”€â”€ macros/                           â†’ Custom SQL macros for automation
â”‚   â”œâ”€â”€ 1_create_file_format.sql
â”‚   â”œâ”€â”€ 2_create_AWS_stage.sql
â”‚   â”œâ”€â”€ 3_create_control_table.sql
â”‚   â”œâ”€â”€ 4_load_control_table.sql
â”‚   â”œâ”€â”€ 5_create_tracking_table.sql
â”‚   â”œâ”€â”€ 6_call_load_walmart_data.sql
â”‚   â””â”€â”€ generate_schema_name.sql
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ gold/                         â†’ Final reporting views
â”‚   â”‚   â”œâ”€â”€ fuel_price_by_store_year.sql
â”‚   â”‚   â”œâ”€â”€ markdown_sales_year_store.sql
â”‚   â”‚   â”œâ”€â”€ schema.yml
â”‚   â”‚   â”œâ”€â”€ weekly_sales_breakdown.sql
â”‚   â”‚   â”œâ”€â”€ weekly_sales_by_cpi.sql
â”‚   â”‚   â”œâ”€â”€ weekly_sales_by_dept.sql
â”‚   â”‚   â”œâ”€â”€ weekly_sales_by_store.sql
â”‚   â”‚   â”œâ”€â”€ weekly_sales_by_store_size.sql
â”‚   â”‚   â”œâ”€â”€ weekly_sales_by_store_type.sql
â”‚   â”‚   â”œâ”€â”€ weekly_sales_by_store_type_month.sql
â”‚   â”‚   â””â”€â”€ weekly_sales_by_tempature_year.sql
â”‚
â”‚   â””â”€â”€ silver/                       â†’ Cleaned and modeled transformation layer
â”‚       â”œâ”€â”€ schema.yml
â”‚       â”œâ”€â”€ stg_fact_table.sql
â”‚       â”œâ”€â”€ Walmart_date_dim.sql
â”‚       â”œâ”€â”€ Walmart_department_dim.sql
â”‚       â”œâ”€â”€ Walmart_department_fact.sql
â”‚       â””â”€â”€ Walmart_store_dim.sql
â”‚
â”œâ”€â”€ seeds/                            â†’ Optional seed files for reference or bootstrapping
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ snapshots/
â”‚   â””â”€â”€ walmart/
â”‚       â””â”€â”€ walmart_fact_table.sql    â†’ SCD2 snapshot definition for Walmart facts
â”‚
â”œâ”€â”€ snowflake/
â”‚   â”œâ”€â”€ ExampleCharts/
â”‚   â”‚   â””â”€â”€ ExampleCharts.pdf         â†’ Sample output chart (PDF)
â”‚   â”œâ”€â”€ notebook/
â”‚   â”‚   â”œâ”€â”€ Visualize_Walmart_Data.ipynb â†’ Snowflake Python notebook for visualization
â”‚   â”‚   â””â”€â”€ Visualize_Walmart_Data.md    â†’ Markdown version of the notebook
â”‚   â””â”€â”€ StoredProcedure/
â”‚       â””â”€â”€ Load_Walmart_Data.sql     â†’ Stored procedure to dynamically load files
â”‚
â”œâ”€â”€ tests/                            â†’ Test artifacts or sample files
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ dbt_project.yml                   â†’ DBT configuration file
â”œâ”€â”€ package-lock.yml
â”œâ”€â”€ packages.yml                      â†’ DBT package dependencies
â””â”€â”€ README.md
```

---

## â–¶ï¸ How It Works

1. â˜ï¸ **Upload Data**
   - Add `.csv` files to the S3 bucket (manually or automated).
   - Entries are logged in `control_table` with file name and MD5 hash.

2. â„ï¸ **Run Stored Procedure**
   - `CALL WALMART.BRONZE.LOAD_WALMART_DATA();`
   - Schema is inferred, table created (if not exists), and data loaded into Bronze layer.

3. ğŸ§± **Run DBT Models**
   - Use `dbt run` to transform data into Silver and Gold.
   - Use `dbt snapshot` to track historical changes (SCD2).

4. ğŸ“Š **Visualize in Notebooks**
   - Python code inside Snowflake Notebook generates charts and pivot tables from Gold views.

---

## ğŸ” SCD Types

### ğŸ”¹ SCD1 (Overwrite)
- Implemented for attributes where history isnâ€™t needed.
- Only current state is retained.

### ğŸ”¸ SCD2 (Track History)
- Implemented via `snapshots/` in dbt.
- Tracks changes by inserting new rows with:
  - `valid_from`
  - `valid_to`
  - `is_current` flag

---

## âœ… Highlights

- âœ… Modular ELT pipeline using dbt's layered architecture
- âœ… Dynamic table creation using `INFER_SCHEMA` in Snowflake SQL
- âœ… SCD1 + SCD2 implementation for real-world use cases
- âœ… End-to-end metadata logging using control and tracking tables
- âœ… Clean charts and tables using Snowflake Notebooks & Python

---

## ğŸ·ï¸ Tags & Topics
```
Use these hashtags when sharing the project:
#DataEngineering #Snowflake #DBT #AWS #S3 #SCD1 #SCD2 #ETL #Snapshots #Notebooks #Analytics #CloudWarehouse
```

